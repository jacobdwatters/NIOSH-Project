{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacobdwatters/NIOSH-Project/blob/main/BreakpointClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qQAb1vn7x-qO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.random import seed\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import r2_score, roc_auc_score, mean_absolute_error, mean_squared_error, recall_score, f1_score, precision_score\n",
        "from numpy.linalg import lstsq\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "\n",
        "import scipy as sp\n",
        "from scipy import stats\n",
        "\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXXdrBnAyR7Q",
        "outputId": "accf35a7-dc14-4945-bb28-457d885daabe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_after_2010 = '/content/gdrive/My Drive/School/Grad School/NIOSH Project/Data/violations_processed_after_2010.csv'\n",
        "violation_data = pd.read_csv(path_after_2010)"
      ],
      "metadata": {
        "id": "ZRPzYrbmyZjj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e82563a-5e55-447e-8d28-9c2fc626446c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-837e5d150e00>:2: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  violation_data = pd.read_csv(path_after_2010)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FEATURES = ['YEAR_OCCUR', 'VIOLATOR_VIOLATION_CNT',\n",
        "'VIOLATOR_INSPECTION_DAY_CNT', 'MINE_TYPE',\n",
        "'PRIMARY_OR_MILL', 'COAL_METAL_IND', 'VIOLATOR_TYPE_CD', 'NO_AFFECTED', 'SIG_SUB', 'NEGLIGENCE', 'INJ_ILLNESS']\n",
        "TARGETS = ['PROPOSED_PENALTY']\n",
        "\n",
        "X = violation_data[FEATURES]\n",
        "y = violation_data[TARGETS]"
      ],
      "metadata": {
        "id": "HuBm5XevyZoT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define which columns should be encoded vs scaled\n",
        "columns_to_encode = ['MINE_TYPE', 'COAL_METAL_IND', 'PRIMARY_OR_MILL', 'VIOLATOR_TYPE_CD', 'SIG_SUB', 'NEGLIGENCE', 'INJ_ILLNESS']\n",
        "columns_to_scale  = ['VIOLATOR_VIOLATION_CNT', 'VIOLATOR_INSPECTION_DAY_CNT', 'YEAR_OCCUR', 'NO_AFFECTED']\n",
        "\n",
        "# Instantiate encoder/scaler\n",
        "scaler = StandardScaler()\n",
        "ohe = OneHotEncoder(sparse=False)\n",
        "\n",
        "y = np.array(y)\n",
        "\n",
        "# Scale and Encode Separate Columns\n",
        "scaled_columns  = scaler.fit_transform(X[columns_to_scale])\n",
        "encoded_columns = ohe.fit_transform(X[columns_to_encode])\n",
        "\n",
        "# Concatenate (Column-Bind) Processed Columns Back Together\n",
        "X_pre = np.concatenate([scaled_columns, encoded_columns], axis=1)\n",
        "np.nan_to_num(X_pre, copy=False)\n",
        "\n",
        "print('Features shape:', X_pre.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HM8cxdMxyiDX",
        "outputId": "ce1aa574-5d8d-46da-8b96-ca1e9b3d10aa"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features shape: (1530011, 25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_bins = 1000\n",
        "hist_freq, hist_ends = np.histogram(y.T[0], bins=n_bins)\n",
        "# get midpoints\n",
        "hist_x = (hist_ends[:n_bins] + hist_ends[1:]) / 2\n",
        "len(hist_x), len(hist_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ybgq33Uym9s",
        "outputId": "e40d9330-fea4-47cd-8ad9-21415092ef64"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(hist_x, hist_freq, 'ok')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "ngKFuPoZ08QR",
        "outputId": "869b505f-d09f-4a2a-cee4-835bcef93c6d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7885cdcd0f10>]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# source: https://datascience.stackexchange.com/a/32833\n",
        "\n",
        "ramp = lambda u: np.maximum( u, 0 )\n",
        "step = lambda u: ( u > 0 ).astype(float)\n",
        "\n",
        "def SegmentedLinearReg( X, Y, breakpoints ):\n",
        "    nIterationMax = 10\n",
        "\n",
        "    breakpoints = np.sort( np.array(breakpoints) )\n",
        "\n",
        "    dt = np.min( np.diff(X) )\n",
        "    ones = np.ones_like(X)\n",
        "\n",
        "    for i in range( nIterationMax ):\n",
        "        # Linear regression:  solve A*p = Y\n",
        "        Rk = [ramp( X - xk ) for xk in breakpoints ]\n",
        "        Sk = [step( X - xk ) for xk in breakpoints ]\n",
        "        A = np.array([ ones, X ] + Rk + Sk )\n",
        "        p =  lstsq(A.transpose(), Y, rcond=None)[0]\n",
        "\n",
        "        # Parameters identification:\n",
        "        a, b = p[0:2]\n",
        "        ck = p[ 2:2+len(breakpoints) ]\n",
        "        dk = p[ 2+len(breakpoints): ]\n",
        "\n",
        "        # Estimation of the next break-points:\n",
        "        newBreakpoints = breakpoints - dk/ck\n",
        "\n",
        "        # Stop condition\n",
        "        if np.max(np.abs(newBreakpoints - breakpoints)) < dt/5:\n",
        "            break\n",
        "\n",
        "        breakpoints = newBreakpoints\n",
        "    else:\n",
        "        print( 'maximum iteration reached' )\n",
        "\n",
        "    # Compute the final segmented fit:\n",
        "    Xsolution = np.insert( np.append( breakpoints, max(X) ), 0, min(X) )\n",
        "    ones =  np.ones_like(Xsolution)\n",
        "    Rk = [ c*ramp( Xsolution - x0 ) for x0, c in zip(breakpoints, ck) ]\n",
        "\n",
        "    Ysolution = a*ones + b*Xsolution + np.sum( Rk, axis=0 )\n",
        "\n",
        "    return Xsolution, Ysolution, breakpoints\n"
      ],
      "metadata": {
        "id": "1UhIgIlp2kX9"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initialBreakpoints = [700]\n",
        "plt.plot(hist_x, hist_freq, 'ok')\n",
        "x_seg, y_seg, breakpoints = SegmentedLinearReg( hist_x, hist_freq, initialBreakpoints )\n",
        "plt.plot(x_seg, y_seg, '-r' );\n",
        "print(f'Breakpoints: {breakpoints}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "cy69PeQP2vOc",
        "outputId": "6870ca36-2931-4299-aaf3-09bc2209c3bb"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Breakpoints: [514.50102434]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi4AAAGsCAYAAAD62iyRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAArWUlEQVR4nO3df3TU1Z3/8VcykAlZSAJGkhACARRQ+SlIDBoLx6wBPbQW2aboCmUrrgpdMPVXFEnR1qgLFE6LxbUi9ZwqKN+Iu8KiGE1FjLJGqCCI8kMTkQQQSfiZwOR+/6AZHUlIJuTHXO7zcc7nnOTzuZ/PvOcOZF7n3vuZCTPGGAEAAFggvK0LAAAAaCyCCwAAsAbBBQAAWIPgAgAArEFwAQAA1iC4AAAAaxBcAACANQguAADAGgQXAABgDYILAACwhlXB5Z133tG4cePUrVs3hYWFaeXKlUFfwxijuXPnqm/fvvJ6vUpKStLvfve75i8WAAA0u3ZtXUAwjh49qsGDB+vf/u3fNH78+CZdY8aMGXrjjTc0d+5cDRw4UAcPHtTBgwebuVIAANASwmz9ksWwsDC98soruvHGG/37qqqq9NBDD+nFF1/UoUOHNGDAAD3xxBMaNWqUJGnbtm0aNGiQtmzZon79+rVN4QAAoMmsmipqyPTp01VUVKRly5bp448/1r/8y79ozJgx+vzzzyVJ//M//6PevXvrtddeU69evZSSkqLbbruNERcAACxx3gSXkpISPffcc3r55ZeVnp6uPn366J577tHVV1+t5557TpK0a9cuffnll3r55Zf1/PPPa+nSpSouLtaECRPauHoAANAYVq1xOZvNmzfL5/Opb9++Afurqqp0wQUXSJJqampUVVWl559/3t/u2Wef1bBhw7R9+3amjwAACHHnTXA5cuSIPB6PiouL5fF4Ao517NhRkpSYmKh27doFhJtLLrlE0ukRG4ILAACh7bwJLkOHDpXP59O+ffuUnp5eZ5urrrpKp06d0s6dO9WnTx9J0meffSZJ6tmzZ6vVCgAAmsaqu4qOHDmiHTt2SDodVObPn6/Ro0erS5cu6tGjh/71X/9V69ev17x58zR06FDt379fBQUFGjRokG644QbV1NToiiuuUMeOHbVgwQLV1NRo2rRpio6O1htvvNHGzw4AADTEquBSWFio0aNHn7F/8uTJWrp0qU6ePKnf/va3ev7557Vnzx7FxcXpyiuv1Jw5czRw4EBJ0tdff61f/epXeuONN/RP//RPGjt2rObNm6cuXbq09tMBAABBsiq4AAAAt503t0MDAIDzH8EFAABYw4q7impqavT111+rU6dOCgsLa+tyAABAIxhjdPjwYXXr1k3h4c0zVmJFcPn666+VnJzc1mUAAIAmKC0tVffu3ZvlWlYEl06dOkk6/cSjo6PbuBoAANAYlZWVSk5O9r+PNwcrgkvt9FB0dDTBBQAAyzTnMg8W5wIAAGsQXAAAgDUILgAAwBoEFwAAYA2CCwAAsAbBBQAAWIPgAgAArEFwAQAA1rDiA+hags/n07p167R3714lJiYqPT1dHo+nrcsCAABn4WRwyc/P14wZM/TVV1/593Xv3l0LFy7U+PHj27AyAABwNs5NFeXn52vChAkBoUWS9uzZowkTJig/P7+NKgMAAA1xKrj4fD7NmDFDxpgzjtXumzlzpnw+X2uXBgAAGsGp4LJu3bqAkZaZkhZIGvCP340xKi0t1bp161q/OAAA0CCngsvevXsDfv+ZpBmSejfQDgAAhAangktiYmKd+3/4Zdv1tQMAAG3LqeCSnp6u7t27KyzsdFT54UqXsLAwJScnKz09vfWLAwAADXIquHg8Hi1cuFCS/OGlVu3vCxYs4PNcAAAIUU4FF0kaP368VqxYoaSkJP++MJ3+HJcVK1bwOS4AAISwMFPXvcEhprKyUjExMaqoqFB0dHSzXNPn8+nI4MGK+eQTbXnkEV3y4IOMtAAA0Ixa4v3byU/OlU5PG8XExEiSBlx2mURoAQAg5Dk3VRQg7If3EwEAgFDmdnABAABWIbhIUugv8wEAAHI9uDBVBACAVdwOLrUYcQEAwApuBxdGXAAAsIrbwQUAAFiF4CIxVQQAgCXcDi5MFQEAYBW3g0stRlwAALCC28GFERcAAKzidnABAABWcTu41I64MFUEAIAV3A4uAADAKgQXiREXAAAsEXRweeeddzRu3Dh169ZNYWFhWrlyZYPnFBYW6vLLL5fX69VFF12kpUuXNqHUFsDiXAAArBJ0cDl69KgGDx6sRYsWNar97t27dcMNN2j06NHatGmTZs6cqdtuu02vv/560MW2GEZcAACwQrtgTxg7dqzGjh3b6PaLFy9Wr169NG/ePEnSJZdconfffVe///3vlZmZGezDNy9GXAAAsEqLr3EpKipSRkZGwL7MzEwVFRXVe05VVZUqKysDNgAAgBYPLmVlZYqPjw/YFx8fr8rKSh0/frzOc/Ly8hQTE+PfkpOTW7ZIpooAALBCSN5VlJOTo4qKCv9WWlraMg/EVBEAAFYJeo1LsBISElReXh6wr7y8XNHR0erQoUOd53i9Xnm93pYu7TuMuAAAYIUWH3FJS0tTQUFBwL61a9cqLS2tpR+6YYy4AABglaCDy5EjR7Rp0yZt2rRJ0unbnTdt2qSSkhJJp6d5Jk2a5G9/xx13aNeuXbrvvvv06aef6qmnntJLL72ku+++u3meAQAAcEbQweXDDz/U0KFDNXToUElSdna2hg4dqtmzZ0uS9u7d6w8xktSrVy+tWrVKa9eu1eDBgzVv3jz9+c9/bvtbob+PqSIAAKwQ9BqXUaNGyZzljb6uT8UdNWqUNm7cGOxDtTymigAAsEpI3lXU6hhxAQDACm4HF0ZcAACwitvBBQAAWIXgIjFVBACAJdwOLkwVAQBgFbeDSy1GXAAAsILbwYURFwAArOJ2cAEAAFZxO7jUjrgwVQQAgBXcDi4AAMAqBBeJERcAACzhdnBhcS4AAFZxO7jUYsQFAAAruB1cGHEBAMAqbgcXAABgFYKLxFQRAACWcDu4MFUEAIBV3A4utRhxAQDACm4HF0ZcAACwitvBBQAAWIXgIjFVBACAJdwOLkwVAQBgFbeDSy1GXAAAsILbwYURFwAArOJ2cAEAAFZxO7jUjrgwVQQAgBXcDi4AAMAqBBeJERcAACzhdnBhcS4AAFZxO7gAAACruB1cWJwLAIBV3A4uAADAKgQXiREXAAAs4XZwYXEuAABWcTu4AAAAq7gdXFicCwCAVdwOLgAAwCoEF4kRFwAALOF2cGFxLgAAVnE7uNRixAUAACu4HVwYcQEAwCpuBxcAAGAVgovEVBEAAJZwO7gwVQQAgFXcDi61GHEBAMAKbgcXRlwAALCK28EFAABYxe3gwncVAQBgFbeDCwAAsArBRWLEBQAAS7gdXFicCwCAVZoUXBYtWqSUlBRFRkYqNTVVGzZsOGv7BQsWqF+/furQoYOSk5N1991368SJE00qGAAAuCvo4LJ8+XJlZ2crNzdXH330kQYPHqzMzEzt27evzvYvvPCCHnjgAeXm5mrbtm169tlntXz5cj344IPnXPw5Y3EuAABWCTq4zJ8/X1OnTtWUKVN06aWXavHixYqKitKSJUvqbP/ee+/pqquu0s0336yUlBRdd911mjhxYoOjNAAAAD8UVHCprq5WcXGxMjIyvrtAeLgyMjJUVFRU5zkjR45UcXGxP6js2rVLq1ev1vXXX1/v41RVVamysjJga1GMuAAAYIV2wTQ+cOCAfD6f4uPjA/bHx8fr008/rfOcm2++WQcOHNDVV18tY4xOnTqlO+6446xTRXl5eZozZ04wpTUNi3MBALBKi99VVFhYqMcee0xPPfWUPvroI+Xn52vVqlV69NFH6z0nJydHFRUV/q20tLSlywQAABYIasQlLi5OHo9H5eXlAfvLy8uVkJBQ5zkPP/ywbr31Vt12222SpIEDB+ro0aO6/fbb9dBDDyk8/Mzs5PV65fV6gymtaVicCwCAVYIacYmIiNCwYcNUUFDg31dTU6OCggKlpaXVec6xY8fOCCcej0eSZAgMAAAgCEGNuEhSdna2Jk+erOHDh2vEiBFasGCBjh49qilTpkiSJk2apKSkJOXl5UmSxo0bp/nz52vo0KFKTU3Vjh079PDDD2vcuHH+ANPmCFAAAFgh6OCSlZWl/fv3a/bs2SorK9OQIUO0Zs0a/4LdkpKSgBGWWbNmKSwsTLNmzdKePXt04YUXaty4cfrd737XfM+iqVicCwCAVcKMBfM1lZWViomJUUVFhaKjo5vvwpMnS88/Lz3xhHTffc13XQAA0CLv33xXEQAAsIbbwQUAAFjF7eDC7dAAAFjF7eACAACsQnCRGHEBAMASbgcXFucCAGAVt4MLAACwitvBhcW5AABYxe3gAgAArEJwkRhxAQDAEm4HFxbnAgBgFbeDCwAAsIrbwYXFuQAAWMXt4AIAAKxCcJEYcQEAwBJuBxcW5wIAYBW3gwsAALCK28GFxbkAAFjF7eACAACsQnCRGHEBAMASbgcXFucCAGAVt4NLLUZcAACwgtvBhREXAACs4nZwAQAAVnE7uHA7NAAAVnE7uAAAAKsQXCRGXAAAsITbwYXFuQAAWMXt4AIAAKzidnBhcS4AAFZxO7gAAACrEFwkRlwAALCE28GFxbkAAFjF7eACAACs4nZwYXEuAABWcTu4AAAAqxBcJEZcAACwhNvBhcW5AABYxe3gAgAArOJ2cGFxLgAAVnE7uAAAAKsQXCRGXAAAsITbwYXFuQAAWMXt4AIAAKzidnBhcS4AAFZxO7gAAACruB1cGHEBAMAqbgcXAABgFYKLxIgLAACWcDu4cDs0AABWcTu4AAAAqzQpuCxatEgpKSmKjIxUamqqNmzYcNb2hw4d0rRp05SYmCiv16u+fftq9erVTSq4WbE4FwAAq7QL9oTly5crOztbixcvVmpqqhYsWKDMzExt375dXbt2PaN9dXW1/vmf/1ldu3bVihUrlJSUpC+//FKxsbHNUT8AAHBI0MFl/vz5mjp1qqZMmSJJWrx4sVatWqUlS5bogQceOKP9kiVLdPDgQb333ntq3769JCklJeXcqm5ujLgAAGCFoKaKqqurVVxcrIyMjO8uEB6ujIwMFRUV1XnOf//3fystLU3Tpk1TfHy8BgwYoMcee0w+n6/ex6mqqlJlZWXA1iJYnAsAgFWCCi4HDhyQz+dTfHx8wP74+HiVlZXVec6uXbu0YsUK+Xw+rV69Wg8//LDmzZun3/72t/U+Tl5enmJiYvxbcnJyMGUCAIDzVIvfVVRTU6OuXbvqv/7rvzRs2DBlZWXpoYce0uLFi+s9JycnRxUVFf6ttLS0ZYpjcS4AAFYJao1LXFycPB6PysvLA/aXl5crISGhznMSExPVvn17eTwe/75LLrlEZWVlqq6uVkRExBnneL1eeb3eYEoDAAAOCGrEJSIiQsOGDVNBQYF/X01NjQoKCpSWllbnOVdddZV27Nihmpoa/77PPvtMiYmJdYaWNsGICwAAVgh6qig7O1vPPPOM/vKXv2jbtm268847dfToUf9dRpMmTVJOTo6//Z133qmDBw9qxowZ+uyzz7Rq1So99thjmjZtWvM9i6ZicS4AAFYJ+nborKws7d+/X7Nnz1ZZWZmGDBmiNWvW+BfslpSUKDz8uzyUnJys119/XXfffbcGDRqkpKQkzZgxQ/fff3/zPQsAAOCEoIOLJE2fPl3Tp0+v81hhYeEZ+9LS0vT+++835aFaFotzAQCwCt9VBAAArOF2cGHEBQAAq7gdXAAAgFUILgAAwBpuBxemigAAsIrbwQUAAFjF7eDCiAsAAFZxO7gAAACrEFwkRlwAALCE28GF7yoCAMAqbgcXAABgFbeDC4tzAQCwitvBBQAAWIXgIjHiAgCAJdwOLizOBQDAKm4HFwAAYBW3gwuLcwEAsIrbwQUAAFiF4CIx4gIAgCXcDi4szgUAwCpuBxcAAGAVt4MLi3MBALCK28EFAABYxe3gwogLAABWcTu4AAAAqxBcAACANdwOLkwVAQBgFbeDCwAAsIrbwYURFwAArOJ2cAEAAFYhuEiMuAAAYAm3gwvfVQQAgFXcDi4AAMAqbgcXFucCAGAVt4MLAACwCsFFYsQFAABLuB1cWJwLAIBV3A4uAADAKm4HFxbnAgBgFbeDCwAAsIrbwYURFwAArOJ2cAEAAFYhuAAAAGu4HVyYKgIAwCpuBxcAAGAVt4MLIy4AAFjF7eACAACsQnABAADWcDu4MFUEAIBV3A4uAADAKm4HF0ZcAACwSpOCy6JFi5SSkqLIyEilpqZqw4YNjTpv2bJlCgsL04033tiUhwUAAI4LOrgsX75c2dnZys3N1UcffaTBgwcrMzNT+/btO+t5X3zxhe655x6lp6c3uVgAAOC2oIPL/PnzNXXqVE2ZMkWXXnqpFi9erKioKC1ZsqTec3w+n2655RbNmTNHvXv3PqeCmxVTRQAAWCWo4FJdXa3i4mJlZGR8d4HwcGVkZKioqKje8x555BF17dpVv/zlLxv1OFVVVaqsrAzYAAAAggouBw4ckM/nU3x8fMD++Ph4lZWV1XnOu+++q2effVbPPPNMox8nLy9PMTEx/i05OTmYMhuPERcAAKzSoncVHT58WLfeequeeeYZxcXFNfq8nJwcVVRU+LfS0tIWrBIAANiiXTCN4+Li5PF4VF5eHrC/vLxcCQkJZ7TfuXOnvvjiC40bN86/r6am5vQDt2un7du3q0+fPmec5/V65fV6gynt3DDiAgCAFYIacYmIiNCwYcNUUFDg31dTU6OCggKlpaWd0b5///7avHmzNm3a5N9+/OMfa/To0dq0aVPLTQE1Vu1UEQAAsEJQIy6SlJ2drcmTJ2v48OEaMWKEFixYoKNHj2rKlCmSpEmTJikpKUl5eXmKjIzUgAEDAs6PjY2VpDP2AwAANCTo4JKVlaX9+/dr9uzZKisr05AhQ7RmzRr/gt2SkhKFh1vygbwszgUAwCpBBxdJmj59uqZPn17nscLCwrOeu3Tp0qY8JAAAAN9VJIkRFwAALOF2cAEAAFYhuAAAAGu4HVyYKgIAwCpuBxcAAGAVt4MLIy4AAFjF7eACAACsQnABAADWcDu4MFUEAIBV3A4uAADAKm4HF0ZcAACwitvBBQAAWIXgAgAArOF2cGGqCAAAq7gdXAAAgFXcDi6MuAAAYBW3gwsAALCK28GFERcAAKzidnABAABWIbgAAABruB1cmCoCAMAqbgcXAABgFbeDCyMuAABYxe3gAgAArEJwAQAA1nA7uDBVBACAVdwOLgAAwCpuBxdGXAAAsIrbwQUAAFiF4AIAAKzhdnBhqggAAKu4HVwAAIBV3A4ujLgAAGAVt4MLAACwCsEFAABYw+3gwlQRAABWcTu4AAAAq7gdXBhxAQDAKm4HFwAAYBW3g0vtiAsAALCC28GlFlNFAABYgeACAACs4XZwYXEuAABWcTu4AAAAq7gdXBhxAQDAKm4HFwAAYBWCCwAAsIbbwYWpIgAArOJ2cAEAAFZxO7gw4gIAgFXcDi4AAMAqTQouixYtUkpKiiIjI5WamqoNGzbU2/aZZ55Renq6OnfurM6dOysjI+Os7QEAAOoTdHBZvny5srOzlZubq48++kiDBw9WZmam9u3bV2f7wsJCTZw4UW+//baKioqUnJys6667Tnv27Dnn4s8ZU0UAAFgl6OAyf/58TZ06VVOmTNGll16qxYsXKyoqSkuWLKmz/V//+lfdddddGjJkiPr3768///nPqqmpUUFBwTkXDwAA3BJUcKmurlZxcbEyMjK+u0B4uDIyMlRUVNSoaxw7dkwnT55Uly5d6m1TVVWlysrKgK1FMOICAIBVggouBw4ckM/nU3x8fMD++Ph4lZWVNeoa999/v7p16xYQfn4oLy9PMTEx/i05OTmYMgEAwHmqVe8qevzxx7Vs2TK98sorioyMrLddTk6OKioq/FtpaWkrVgkAAEJVu2Aax8XFyePxqLy8PGB/eXm5EhISznru3Llz9fjjj+vNN9/UoEGDztrW6/XK6/UGU1rTMFUEAIBVghpxiYiI0LBhwwIW1tYutE1LS6v3vCeffFKPPvqo1qxZo+HDhze9WgAA4LSgRlwkKTs7W5MnT9bw4cM1YsQILViwQEePHtWUKVMkSZMmTVJSUpLy8vIkSU888YRmz56tF154QSkpKf61MB07dlTHjh2b8ak0ASMuAABYJejgkpWVpf3792v27NkqKyvTkCFDtGbNGv+C3ZKSEoWHfzeQ86c//UnV1dWaMGFCwHVyc3P1m9/85tyqP0e+mhp5JO3bt09bCwuVnp4uj8fTpjUBAID6hRkT+sMNlZWViomJUUVFhaKjo5vlmvn5+Xp96lQ9ffCg/iZplKTu3btr4cKFGj9+fLM8BgAALmuJ928nv6soPz9fEyZM0DcHD0qS/jFhpD179mjChAnKz89vu+IAAEC9nAsuPp9PM2bMUF0DTbX7Zs6cKZ/P19qlAQCABjgXXNatW6evvvpKklQbXcK+d9wYo9LSUq1bt67VawMAAGfnXHDZu3dvs7YDAACtx7ngkpiY6P+5rhGXutoBAIDQ4FxwSU9PV/fu3RUWVldckcLCwpScnKz09PRWrgwAADTEueDi8Xi0cOFCSWeOtNSGmQULFvB5LgAAhCDngoskjR8/XitWrFCXCy6Q9F2A6d69u1asWMHnuAAAEKKC/uTc88X48eP1E2OkCRPUt29fvf3003xyLgAAIc7Z4CLJH1IuvOACjRo1qm2LAQAADXJyqggAANiJ4AIAAKzhdnCpvSU69L9nEgAAyPXgAgAArOJ2cGHEBQAAq7gdXAAAgFXcDi71fOw/AAAITW4Hl1pMFQEAYAWCCwAAsIbbwYXFuQAAWMXt4AIAAKzidnBhcS4AAFZxO7jUYqoIAAArEFwAAIA13A4uLM4FAMAqbgcXAABgFbeDCyMuAABYxe3gAgAArEJwAQAA1nA7uDBVBACAVZwOLj6fT5J08NtvVVhY6P8dAACEJmeDS35+vib/4heSpC+/+EKjR49WSkqK8vPz27YwAABQLyeDS35+viZMmKD9Bw4E7N+zZ48mTJhAeAEAIEQ5F1x8Pp9mzJghU8e6ltp9M2fOZNoIAIAQ5FxwWbdunb766itJUm10+f5XLRpjVFpaqnXr1rV6bQAA4OycCy579+5t1nYAAKD1OBdcEhMT/T/XNeJSVzsAABAanAsu6enp6t69u8LC6oorpyUnJys9Pb0VqwIAAI3hXHDxeDxauHChpO9GXH7o5z//uTweT+sVBQAAGsW54CJJ48eP1z333OP//YdjL3PnzuWWaAAAQpCTwcXn8+nFF188axtuiQYAIPQ4GVxqb4mub3Eut0QDABCanAwu3BINAICdnAwutbc6N/Sd0NwSDQBAaHEyuIwcOTLgrqG6boz2eDwaOXJk6xUFAAAa5GRwee+99xpceOvz+fTee++1UkUAAKAxnAwutWtXzvbJuZL06quvtko9AACgcZwMLo1du/L0009zSzQAACHEyeCSnp6uCy64oMHFucePH9ejjz7aKjUBAICGORlcPB6PfvSjH/l/r/9bi6Q5c+bo9ddfZ+QFAIAQ0K6tC2grUVFR+qaRbceMGSNJ6tSpk9q3by+Px6PIyEhJ0okTJ+Tz+eTxeOT1elVVVeX/va42rX1eKNbEc6EPQrUm+oDnEqo1teR5UVFRSkpK0k9/+lP9x3/8hyIiIhTSTBP88Y9/ND179jRer9eMGDHCfPDBB2dt/9JLL5l+/foZr9drBgwYYFatWhXU41VUVBhJpqKioinl1unBBx8010jGSGbr6XW6bGxsbGxsTm9hYWHm3nvvbbb32pZ4/w46uCxbtsxERESYJUuWmE8++cRMnTrVxMbGmvLy8jrbr1+/3ng8HvPkk0+arVu3mlmzZpn27dubzZs3N/oxW+KJv/nmmwQXNjY2Nja2OrbmCi8t8f4dZowxCkJqaqquuOIK/fGPf5Qk1dTUKDk5Wb/61a/0wAMPnNE+KytLR48e1Wuvvebfd+WVV2rIkCFavHhxox6zsrJSMTExqqioUHR0dDDl1svn8+n6jh31+okT+lxS6g+Om3p+bu1jrfHYAAB8n8fj0bFjx8552qgl3r+DWuNSXV2t4uJi5eTk+PeFh4crIyNDRUVFdZ5TVFSk7OzsgH2ZmZlauXJlvY9TVVWlqqoq/++VlZXBlNkoHo9HEydOlJ57ThdLOtjsj3B+qPnez6EUts638EjNZz8WqnWd7Vio1tXUY6FaV1OPhWpdZzvW0tf/vaQv//Gzz+fTU089pZkzZyrUBBVcDhw4IJ/Pp/j4+ID98fHx+vTTT+s8p6ysrM72ZWVl9T5OXl6e5syZE0xpTXLrk0+q5C9/UY+amoYbO8rJ284AwEEv6rvgIkk7d+5sq1LOKiTvKsrJyQkYpamsrFRycnKzP44nLk4fvvSSek2Y4N/3/Vujf3ibdGsea8vHDqVjoVpXax8L1bpa4lio1tXax0K1rpY4Fqp1tfaxtq5rzw9+79Onj0JRUMElLi5OHo9H5eXlAfvLy8uVkJBQ5zkJCQlBtZckr9crr9cbTGlNNv6mm/Ty//t/ysrK0qlTp1rlMQEACGUej0d33XVXW5dRp6BmAiIiIjRs2DAVFBT499XU1KigoEBpaWl1npOWlhbQXpLWrl1bb/u2MH78eJ04cSKkagIAoK1kZ2eH7ue5BHsb0rJly4zX6zVLly41W7duNbfffruJjY01ZWVlxhhjbr31VvPAAw/4269fv960a9fOzJ0712zbts3k5uaGxO3Q9Vm+fLmJiooyktjY2NjY2JzabPgcl6DXuGRlZWn//v2aPXu2ysrKNGTIEK1Zs8a/ALekpETh4d8N5IwcOVIvvPCCZs2apQcffFAXX3yxVq5cqQEDBgT70K3iZz/7mW666SYVFhbqzTff1IYNG1RWVqbDhw+H/Kcf2lITz4U+CNWa6AOeS6jWxCfnfifoz3FpCy1xHzgAAGhZLfH+zd2uAADAGgQXAABgDYILAACwBsEFAABYg+ACAACsQXABAADWILgAAABrEFwAAIA1CC4AAMAaQX/kf1uo/XDfysrKNq4EAAA0Vu37dnN+SL8VweXw4cOSpOTk5DauBAAABOvw4cOKiYlplmtZ8V1FNTU1+vrrr9WpUyeFhYU123UrKyuVnJys0tJSvgOpFdDfrYe+bl30d+uiv1vPufa1MUaHDx9Wt27dAr6A+VxYMeISHh6u7t27t9j1o6Oj+cffiujv1kNfty76u3XR363nXPq6uUZaarE4FwAAWIPgAgAArOF0cPF6vcrNzZXX623rUpxAf7ce+rp10d+ti/5uPaHY11YszgUAAJAcH3EBAAB2IbgAAABrEFwAAIA1CC4AAMAazgaXRYsWKSUlRZGRkUpNTdWGDRvauqSQ85vf/EZhYWEBW//+/f3HT5w4oWnTpumCCy5Qx44dddNNN6m8vDzgGiUlJbrhhhsUFRWlrl276t5779WpU6cC2hQWFuryyy+X1+vVRRddpKVLl55Ry/n4er3zzjsaN26cunXrprCwMK1cuTLguDFGs2fPVmJiojp06KCMjAx9/vnnAW0OHjyoW265RdHR0YqNjdUvf/lLHTlyJKDNxx9/rPT0dEVGRio5OVlPPvnkGbW8/PLL6t+/vyIjIzVw4ECtXr066FpCWUN9/Ytf/OKMf+tjxowJaENfN05eXp6uuOIKderUSV27dtWNN96o7du3B7QJpb8djakllDWmv0eNGnXGv+877rgjoI1V/W0ctGzZMhMREWGWLFliPvnkEzN16lQTGxtrysvL27q0kJKbm2suu+wys3fvXv+2f/9+//E77rjDJCcnm4KCAvPhhx+aK6+80owcOdJ//NSpU2bAgAEmIyPDbNy40axevdrExcWZnJwcf5tdu3aZqKgok52dbbZu3Wr+8Ic/GI/HY9asWeNvc76+XqtXrzYPPfSQyc/PN5LMK6+8EnD88ccfNzExMWblypXm73//u/nxj39sevXqZY4fP+5vM2bMGDN48GDz/vvvm3Xr1pmLLrrITJw40X+8oqLCxMfHm1tuucVs2bLFvPjii6ZDhw7m6aef9rdZv3698Xg85sknnzRbt241s2bNMu3btzebN28OqpZQ1lBfT5482YwZMybg3/rBgwcD2tDXjZOZmWmee+45s2XLFrNp0yZz/fXXmx49epgjR47424TS346Gagl1jenvH/3oR2bq1KkB/74rKir8x23rbyeDy4gRI8y0adP8v/t8PtOtWzeTl5fXhlWFntzcXDN48OA6jx06dMi0b9/evPzyy/5927ZtM5JMUVGRMeb0m0V4eLgpKyvzt/nTn/5koqOjTVVVlTHGmPvuu89cdtllAdfOysoymZmZ/t9deL1++GZaU1NjEhISzH/+53/69x06dMh4vV7z4osvGmOM2bp1q5Fk/u///s/f5n//939NWFiY2bNnjzHGmKeeesp07tzZ39/GGHP//febfv36+X//2c9+Zm644YaAelJTU82///u/N7oWm9QXXH7yk5/Uew593XT79u0zkszf/vY3Y0xo/e1oTC22+WF/G3M6uMyYMaPec2zrb+emiqqrq1VcXKyMjAz/vvDwcGVkZKioqKgNKwtNn3/+ubp166bevXvrlltuUUlJiSSpuLhYJ0+eDOjH/v37q0ePHv5+LCoq0sCBAxUfH+9vk5mZqcrKSn3yySf+Nt+/Rm2b2mu4+nrt3r1bZWVlAc87JiZGqampAf0bGxur4cOH+9tkZGQoPDxcH3zwgb/NNddco4iICH+bzMxMbd++Xd9++62/zdleg8bUcj4oLCxU165d1a9fP91555365ptv/Mfo66arqKiQJHXp0kVSaP3taEwttvlhf9f661//qri4OA0YMEA5OTk6duyY/5ht/W3Flyw2pwMHDsjn8wW8QJIUHx+vTz/9tI2qCk2pqalaunSp+vXrp71792rOnDlKT0/Xli1bVFZWpoiICMXGxgacEx8fr7KyMklSWVlZnf1ce+xsbSorK3X8+HF9++23Tr5etf1T1/P+ft917do14Hi7du3UpUuXgDa9evU64xq1xzp37lzva/D9azRUi+3GjBmj8ePHq1evXtq5c6cefPBBjR07VkVFRfJ4PPR1E9XU1GjmzJm66qqrNGDAAEkKqb8djanFJnX1tyTdfPPN6tmzp7p166aPP/5Y999/v7Zv3678/HxJ9vW3c8EFjTd27Fj/z4MGDVJqaqp69uypl156SR06dGjDyoDm9fOf/9z/88CBAzVo0CD16dNHhYWFuvbaa9uwMrtNmzZNW7Zs0bvvvtvWpTihvv6+/fbb/T8PHDhQiYmJuvbaa7Vz50716dOntcs8Z85NFcXFxcnj8Zyxirm8vFwJCQltVJUdYmNj1bdvX+3YsUMJCQmqrq7WoUOHAtp8vx8TEhLq7OfaY2drEx0drQ4dOjj7etU+t7M974SEBO3bty/g+KlTp3Tw4MFmeQ2+f7yhWs43vXv3VlxcnHbs2CGJvm6K6dOn67XXXtPbb7+t7t27+/eH0t+OxtRii/r6uy6pqamSFPDv26b+di64REREaNiwYSooKPDvq6mpUUFBgdLS0tqwstB35MgR7dy5U4mJiRo2bJjat28f0I/bt29XSUmJvx/T0tK0efPmgD/4a9euVXR0tC699FJ/m+9fo7ZN7TVcfb169eqlhISEgOddWVmpDz74IKB/Dx06pOLiYn+bt956SzU1Nf4/TGlpaXrnnXd08uRJf5u1a9eqX79+6ty5s7/N2V6DxtRyvvnqq6/0zTffKDExURJ9HQxjjKZPn65XXnlFb7311hnTZ6H0t6MxtYS6hvq7Lps2bZKkgH/fVvV3o5fxnkeWLVtmvF6vWbp0qdm6dau5/fbbTWxsbMCKahjz61//2hQWFprdu3eb9evXm4yMDBMXF2f27dtnjDl9W1uPHj3MW2+9ZT788EOTlpZm0tLS/OfX3mJ33XXXmU2bNpk1a9aYCy+8sM5b7O69916zbds2s2jRojpvsTsfX6/Dhw+bjRs3mo0bNxpJZv78+Wbjxo3myy+/NMacvi02NjbWvPrqq+bjjz82P/nJT+q8HXro0KHmgw8+MO+++665+OKLA27RPXTokImPjze33nqr2bJli1m2bJmJioo64xbddu3amblz55pt27aZ3NzcOm/RbaiWUHa2vj58+LC55557TFFRkdm9e7d58803zeWXX24uvvhic+LECf816OvGufPOO01MTIwpLCwMuP322LFj/jah9LejoVpCXUP9vWPHDvPII4+YDz/80Ozevdu8+uqrpnfv3uaaa67xX8O2/nYyuBhjzB/+8AfTo0cPExERYUaMGGHef//9ti4p5GRlZZnExEQTERFhkpKSTFZWltmxY4f/+PHjx81dd91lOnfubKKiosxPf/pTs3fv3oBrfPHFF2bs2LGmQ4cOJi4uzvz61782J0+eDGjz9ttvmyFDhpiIiAjTu3dv89xzz51Ry/n4er399ttG0hnb5MmTjTGnb419+OGHTXx8vPF6vebaa68127dvD7jGN998YyZOnGg6duxooqOjzZQpU8zhw4cD2vz97383V199tfF6vSYpKck8/vjjZ9Ty0ksvmb59+5qIiAhz2WWXmVWrVgUcb0wtoexsfX3s2DFz3XXXmQsvvNC0b9/e9OzZ00ydOvWMYExfN05d/Swp4P91KP3taEwtoayh/i4pKTHXXHON6dKli/F6veaiiy4y9957b8DnuBhjV3+H/eOJAwAAhDzn1rgAAAB7EVwAAIA1CC4AAMAaBBcAAGANggsAALAGwQUAAFiD4AIAAKxBcAEAANYguAAAAGsQXAAAgDUILgAAwBoEFwAAYI3/D7O5a4dNCzHKAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_categories = y > breakpoints[0]\n",
        "pd.Series(y_categories.T[0]).value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pk-0woI44W4w",
        "outputId": "5f9f1be7-0e16-4372-9355-aa906bd496e0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False    1180904\n",
              "True      349107\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_pre, y_categories, test_size = 0.25, random_state = 0)\n",
        "\n",
        "print('X_train shape:', X_train.shape)\n",
        "print('X_test shape:', X_test.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_train shape:', y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pSEmuid5-j4",
        "outputId": "c0e23a13-26a2-418b-e831-ac2d690c4c57"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (1147508, 25)\n",
            "X_test shape: (382503, 25)\n",
            "y_train shape: (1147508, 1)\n",
            "y_train shape: (382503, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2**14\n",
        "epochs = 120\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "\n",
        "model_breakpoint = keras.Sequential()\n",
        "\n",
        "model_breakpoint.add(keras.Input(shape = (len(X_train[0]), ) ))\n",
        "model_breakpoint.add(layers.Dense(64, activation=\"relu\"))\n",
        "model_breakpoint.add(layers.Dense(32, activation=\"relu\"))\n",
        "model_breakpoint.add(layers.Dense(16, activation=\"relu\"))\n",
        "model_breakpoint.add(layers.Dense(8, activation=\"relu\"))\n",
        "model_breakpoint.add(layers.Dense(8, activation=\"relu\"))\n",
        "model_breakpoint.add(layers.Dense(8, activation=\"relu\"))\n",
        "model_breakpoint.add(layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "\n",
        "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "model_breakpoint.compile(loss=\"binary_crossentropy\", optimizer=opt)\n",
        "history = model_breakpoint.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), callbacks=[callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_42zA1i049tK",
        "outputId": "74f44d42-8209-4526-c428-13de597e06c8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/120\n",
            "71/71 [==============================] - 3s 27ms/step - loss: 0.3354 - val_loss: 0.2365\n",
            "Epoch 2/120\n",
            "71/71 [==============================] - 2s 28ms/step - loss: 0.2296 - val_loss: 0.2263\n",
            "Epoch 3/120\n",
            "71/71 [==============================] - 3s 36ms/step - loss: 0.2091 - val_loss: 0.2015\n",
            "Epoch 4/120\n",
            "71/71 [==============================] - 2s 26ms/step - loss: 0.2111 - val_loss: 0.2137\n",
            "Epoch 5/120\n",
            "71/71 [==============================] - 2s 24ms/step - loss: 0.1993 - val_loss: 0.2019\n",
            "Epoch 6/120\n",
            "71/71 [==============================] - 2s 22ms/step - loss: 0.1898 - val_loss: 0.1907\n",
            "Epoch 7/120\n",
            "71/71 [==============================] - 2s 22ms/step - loss: 0.1881 - val_loss: 0.1893\n",
            "Epoch 8/120\n",
            "71/71 [==============================] - 2s 22ms/step - loss: 0.1851 - val_loss: 0.2120\n",
            "Epoch 9/120\n",
            "71/71 [==============================] - 2s 22ms/step - loss: 0.1958 - val_loss: 0.1864\n",
            "Epoch 10/120\n",
            "71/71 [==============================] - 2s 33ms/step - loss: 0.1850 - val_loss: 0.1879\n",
            "Epoch 11/120\n",
            "71/71 [==============================] - 2s 33ms/step - loss: 0.1834 - val_loss: 0.1931\n",
            "Epoch 12/120\n",
            "71/71 [==============================] - 2s 22ms/step - loss: 0.1834 - val_loss: 0.1867\n",
            "Epoch 13/120\n",
            "71/71 [==============================] - 2s 22ms/step - loss: 0.1812 - val_loss: 0.1834\n",
            "Epoch 14/120\n",
            "71/71 [==============================] - 2s 22ms/step - loss: 0.1804 - val_loss: 0.2038\n",
            "Epoch 15/120\n",
            "71/71 [==============================] - 2s 22ms/step - loss: 0.1810 - val_loss: 0.1841\n",
            "Epoch 16/120\n",
            "71/71 [==============================] - 2s 24ms/step - loss: 0.1809 - val_loss: 0.1791\n",
            "Epoch 17/120\n",
            "71/71 [==============================] - 2s 22ms/step - loss: 0.1794 - val_loss: 0.1790\n",
            "Epoch 18/120\n",
            "71/71 [==============================] - 2s 35ms/step - loss: 0.1802 - val_loss: 0.1954\n",
            "Epoch 19/120\n",
            "71/71 [==============================] - 2s 30ms/step - loss: 0.1774 - val_loss: 0.1820\n",
            "Epoch 20/120\n",
            "71/71 [==============================] - 2s 24ms/step - loss: 0.1772 - val_loss: 0.1767\n",
            "Epoch 21/120\n",
            "71/71 [==============================] - 2s 22ms/step - loss: 0.1774 - val_loss: 0.1791\n",
            "Epoch 22/120\n",
            "71/71 [==============================] - 2s 22ms/step - loss: 0.1752 - val_loss: 0.1752\n",
            "Epoch 23/120\n",
            "71/71 [==============================] - 2s 22ms/step - loss: 0.1778 - val_loss: 0.1805\n",
            "Epoch 24/120\n",
            "71/71 [==============================] - 2s 22ms/step - loss: 0.1756 - val_loss: 0.1847\n",
            "Epoch 25/120\n",
            "71/71 [==============================] - 2s 24ms/step - loss: 0.1761 - val_loss: 0.1770\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model_breakpoint.predict(X_test)\n",
        "print(tf.metrics.binary_accuracy(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KX-wvxuj6pKy",
        "outputId": "9ab89a4d-2c6a-4613-b688-45a58b009c7b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11954/11954 [==============================] - 20s 2ms/step\n",
            "tf.Tensor([1. 1. 1. ... 1. 1. 1.], shape=(382503,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bin_acc = tf.metrics.binary_accuracy(y_test, y_pred)\n",
        "print(np.sum(bin_acc) / len(bin_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjJTqpdD7xG4",
        "outputId": "e80a0855-a6e1-42fb-ffca-2c6e250bf9aa"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9301103520756699\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_lower = X_pre[(y < breakpoints[0])[:, 0]]\n",
        "y_lower = y[y < breakpoints[0]]\n",
        "X_upper = X_pre[(y >= breakpoints[0])[:, 0]]\n",
        "y_upper = y[y >= breakpoints[0]]\n",
        "\n",
        "X_train_lower, X_test_lower, y_train_lower, y_test_lower = train_test_split(X_lower, y_lower, test_size = 0.25, random_state = 0)\n",
        "X_train_upper, X_test_upper, y_train_upper, y_test_upper = train_test_split(X_upper, y_upper, test_size = 0.25, random_state = 0)\n",
        "\n",
        "print('X_train_lower shape:', X_train_lower.shape)\n",
        "print('X_test_lower shape:', X_test_lower.shape)\n",
        "print('y_train_lower shape:', y_train_lower.shape)\n",
        "print('y_train_lower shape:', y_test_lower.shape)\n",
        "\n",
        "print('X_train_upper shape:', X_train_upper.shape)\n",
        "print('X_test_upper shape:', X_test_upper.shape)\n",
        "print('y_train_upper shape:', y_train_upper.shape)\n",
        "print('y_train_upper shape:', y_test_upper.shape)"
      ],
      "metadata": {
        "id": "RI41LwkfKplx",
        "outputId": "c9290207-b9c6-4905-9800-4df74364622c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train_lower shape: (885678, 25)\n",
            "X_test_lower shape: (295226, 25)\n",
            "y_train_lower shape: (885678,)\n",
            "y_train_lower shape: (295226,)\n",
            "X_train_upper shape: (261830, 25)\n",
            "X_test_upper shape: (87277, 25)\n",
            "y_train_upper shape: (261830,)\n",
            "y_train_upper shape: (87277,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scheduler(epoch, lr):\n",
        "  if epoch < 85:\n",
        "    return lr\n",
        "  else: # Drop learning rate after the first 85 epochs\n",
        "    return lr*np.exp(-0.05)"
      ],
      "metadata": {
        "id": "DaIpuwCcP2wi"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "batch_size = 2**14\n",
        "epochs = 120\n",
        "callback_lower = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "model_lower = keras.Sequential()\n",
        "\n",
        "model_lower.add(keras.Input(shape = (len(X_train[0]), ) ))\n",
        "model_lower.add(layers.Dense(64, activation=\"relu\"))\n",
        "model_lower.add(layers.Dense(32, activation=\"relu\"))\n",
        "model_lower.add(layers.Dense(16, activation=\"relu\"))\n",
        "model_lower.add(layers.Dense(8, activation=\"relu\"))\n",
        "model_lower.add(layers.Dense(8, activation=\"relu\"))\n",
        "model_lower.add(layers.Dense(8, activation=\"relu\"))\n",
        "model_lower.add(layers.Dense(1, activation=\"linear\"))\n",
        "\n",
        "\n",
        "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "model_lower.compile(loss=\"mse\", optimizer=opt)\n",
        "history_lower = model_lower.fit(X_train_lower, y_train_lower, epochs=epochs, batch_size=batch_size,\n",
        "                    callbacks=[callback_lower], validation_data=(X_test_lower, y_test_lower))"
      ],
      "metadata": {
        "id": "SldHNXiwJu-i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97f68c7e-7a05-433f-f9cf-0b5ecbdcd3d6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/120\n",
            "55/55 [==============================] - 3s 27ms/step - loss: 18566.1113 - val_loss: 6740.2896 - lr: 0.0100\n",
            "Epoch 2/120\n",
            "55/55 [==============================] - 1s 23ms/step - loss: 6062.2466 - val_loss: 5609.6543 - lr: 0.0100\n",
            "Epoch 3/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 5425.1460 - val_loss: 5125.8628 - lr: 0.0100\n",
            "Epoch 4/120\n",
            "55/55 [==============================] - 1s 21ms/step - loss: 5045.3584 - val_loss: 4951.5576 - lr: 0.0100\n",
            "Epoch 5/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 4799.4937 - val_loss: 4682.4507 - lr: 0.0100\n",
            "Epoch 6/120\n",
            "55/55 [==============================] - 2s 33ms/step - loss: 4699.4224 - val_loss: 4724.7349 - lr: 0.0100\n",
            "Epoch 7/120\n",
            "55/55 [==============================] - 2s 38ms/step - loss: 4655.7217 - val_loss: 4613.9219 - lr: 0.0100\n",
            "Epoch 8/120\n",
            "55/55 [==============================] - 1s 23ms/step - loss: 4521.9541 - val_loss: 4458.4082 - lr: 0.0100\n",
            "Epoch 9/120\n",
            "55/55 [==============================] - 1s 23ms/step - loss: 4490.1226 - val_loss: 4589.0181 - lr: 0.0100\n",
            "Epoch 10/120\n",
            "55/55 [==============================] - 1s 21ms/step - loss: 4554.3569 - val_loss: 4449.7319 - lr: 0.0100\n",
            "Epoch 11/120\n",
            "55/55 [==============================] - 1s 23ms/step - loss: 4439.9873 - val_loss: 4542.6436 - lr: 0.0100\n",
            "Epoch 12/120\n",
            "55/55 [==============================] - 1s 23ms/step - loss: 4450.0918 - val_loss: 4391.0273 - lr: 0.0100\n",
            "Epoch 13/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 4399.9624 - val_loss: 4257.6797 - lr: 0.0100\n",
            "Epoch 14/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 4215.1855 - val_loss: 4096.5400 - lr: 0.0100\n",
            "Epoch 15/120\n",
            "55/55 [==============================] - 2s 32ms/step - loss: 3941.2175 - val_loss: 3822.3101 - lr: 0.0100\n",
            "Epoch 16/120\n",
            "55/55 [==============================] - 3s 51ms/step - loss: 3822.1719 - val_loss: 3736.1101 - lr: 0.0100\n",
            "Epoch 17/120\n",
            "55/55 [==============================] - 2s 31ms/step - loss: 3709.5906 - val_loss: 3789.4180 - lr: 0.0100\n",
            "Epoch 18/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 3624.8442 - val_loss: 3557.4316 - lr: 0.0100\n",
            "Epoch 19/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 3545.8562 - val_loss: 3701.1033 - lr: 0.0100\n",
            "Epoch 20/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 3486.0474 - val_loss: 3466.9324 - lr: 0.0100\n",
            "Epoch 21/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 3431.8799 - val_loss: 3394.2983 - lr: 0.0100\n",
            "Epoch 22/120\n",
            "55/55 [==============================] - 2s 29ms/step - loss: 3395.4917 - val_loss: 3648.3755 - lr: 0.0100\n",
            "Epoch 23/120\n",
            "55/55 [==============================] - 1s 23ms/step - loss: 3386.9417 - val_loss: 3446.8430 - lr: 0.0100\n",
            "Epoch 24/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 3361.6211 - val_loss: 3401.5935 - lr: 0.0100\n",
            "Epoch 25/120\n",
            "55/55 [==============================] - 2s 30ms/step - loss: 3315.5391 - val_loss: 3314.8867 - lr: 0.0100\n",
            "Epoch 26/120\n",
            "55/55 [==============================] - 2s 35ms/step - loss: 3260.0588 - val_loss: 3311.3848 - lr: 0.0100\n",
            "Epoch 27/120\n",
            "55/55 [==============================] - 2s 29ms/step - loss: 3265.4294 - val_loss: 3259.6433 - lr: 0.0100\n",
            "Epoch 28/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 3223.8689 - val_loss: 3353.2827 - lr: 0.0100\n",
            "Epoch 29/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 3216.2805 - val_loss: 3624.3076 - lr: 0.0100\n",
            "Epoch 30/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 3308.4287 - val_loss: 3203.8591 - lr: 0.0100\n",
            "Epoch 31/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 3183.7712 - val_loss: 3164.1021 - lr: 0.0100\n",
            "Epoch 32/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 3165.7197 - val_loss: 3153.2275 - lr: 0.0100\n",
            "Epoch 33/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 3139.1272 - val_loss: 3164.0664 - lr: 0.0100\n",
            "Epoch 34/120\n",
            "55/55 [==============================] - 1s 21ms/step - loss: 3112.1025 - val_loss: 3305.0835 - lr: 0.0100\n",
            "Epoch 35/120\n",
            "55/55 [==============================] - 1s 24ms/step - loss: 3125.2871 - val_loss: 3269.3789 - lr: 0.0100\n",
            "Epoch 36/120\n",
            "55/55 [==============================] - 2s 37ms/step - loss: 3112.4023 - val_loss: 3181.2820 - lr: 0.0100\n",
            "Epoch 37/120\n",
            "55/55 [==============================] - 2s 33ms/step - loss: 3073.7898 - val_loss: 3294.3625 - lr: 0.0100\n",
            "Epoch 38/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 3092.3806 - val_loss: 3110.7798 - lr: 0.0100\n",
            "Epoch 39/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 3067.8447 - val_loss: 3093.8342 - lr: 0.0100\n",
            "Epoch 40/120\n",
            "55/55 [==============================] - 1s 21ms/step - loss: 3042.6770 - val_loss: 3091.7930 - lr: 0.0100\n",
            "Epoch 41/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 3081.9846 - val_loss: 3157.3369 - lr: 0.0100\n",
            "Epoch 42/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 3080.5435 - val_loss: 3111.1885 - lr: 0.0100\n",
            "Epoch 43/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 3035.8279 - val_loss: 3061.1621 - lr: 0.0100\n",
            "Epoch 44/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 3013.8569 - val_loss: 3021.3577 - lr: 0.0100\n",
            "Epoch 45/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 3000.4556 - val_loss: 3043.6592 - lr: 0.0100\n",
            "Epoch 46/120\n",
            "55/55 [==============================] - 3s 46ms/step - loss: 3025.3787 - val_loss: 2997.1091 - lr: 0.0100\n",
            "Epoch 47/120\n",
            "55/55 [==============================] - 2s 36ms/step - loss: 3019.0403 - val_loss: 3096.6985 - lr: 0.0100\n",
            "Epoch 48/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 2996.1604 - val_loss: 2994.8318 - lr: 0.0100\n",
            "Epoch 49/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 3004.1228 - val_loss: 3032.5911 - lr: 0.0100\n",
            "Epoch 50/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 3015.1863 - val_loss: 3011.4563 - lr: 0.0100\n",
            "Epoch 51/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 3003.2166 - val_loss: 2994.5083 - lr: 0.0100\n",
            "Epoch 52/120\n",
            "55/55 [==============================] - 1s 24ms/step - loss: 2974.7849 - val_loss: 3017.9851 - lr: 0.0100\n",
            "Epoch 53/120\n",
            "55/55 [==============================] - 1s 23ms/step - loss: 2987.6038 - val_loss: 2984.8972 - lr: 0.0100\n",
            "Epoch 54/120\n",
            "55/55 [==============================] - 1s 23ms/step - loss: 2991.9412 - val_loss: 3195.9353 - lr: 0.0100\n",
            "Epoch 55/120\n",
            "55/55 [==============================] - 1s 26ms/step - loss: 2993.9700 - val_loss: 3040.1738 - lr: 0.0100\n",
            "Epoch 56/120\n",
            "55/55 [==============================] - 2s 38ms/step - loss: 2977.5022 - val_loss: 2987.6045 - lr: 0.0100\n",
            "Epoch 57/120\n",
            "55/55 [==============================] - 2s 30ms/step - loss: 2962.9631 - val_loss: 3014.2290 - lr: 0.0100\n",
            "Epoch 58/120\n",
            "55/55 [==============================] - 1s 23ms/step - loss: 2960.0820 - val_loss: 2963.6978 - lr: 0.0100\n",
            "Epoch 59/120\n",
            "55/55 [==============================] - 1s 23ms/step - loss: 3008.6267 - val_loss: 2977.0476 - lr: 0.0100\n",
            "Epoch 60/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 2974.8914 - val_loss: 3003.6272 - lr: 0.0100\n",
            "Epoch 61/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 2963.7285 - val_loss: 3072.5789 - lr: 0.0100\n",
            "Epoch 62/120\n",
            "55/55 [==============================] - 1s 23ms/step - loss: 2970.0886 - val_loss: 3130.9299 - lr: 0.0100\n",
            "Epoch 63/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 2982.7273 - val_loss: 2993.2388 - lr: 0.0100\n",
            "Epoch 64/120\n",
            "55/55 [==============================] - 1s 21ms/step - loss: 2939.2007 - val_loss: 2976.5415 - lr: 0.0100\n",
            "Epoch 65/120\n",
            "55/55 [==============================] - 1s 24ms/step - loss: 2964.2048 - val_loss: 3068.5010 - lr: 0.0100\n",
            "Epoch 66/120\n",
            "55/55 [==============================] - 2s 35ms/step - loss: 2954.7437 - val_loss: 2983.4150 - lr: 0.0100\n",
            "Epoch 67/120\n",
            "55/55 [==============================] - 2s 35ms/step - loss: 2942.5771 - val_loss: 2997.4546 - lr: 0.0100\n",
            "Epoch 68/120\n",
            "55/55 [==============================] - 1s 23ms/step - loss: 2951.5989 - val_loss: 2987.3176 - lr: 0.0100\n",
            "Epoch 69/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 2938.1440 - val_loss: 3102.1069 - lr: 0.0100\n",
            "Epoch 70/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 2945.9355 - val_loss: 2954.2039 - lr: 0.0100\n",
            "Epoch 71/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 2931.6873 - val_loss: 3030.3660 - lr: 0.0100\n",
            "Epoch 72/120\n",
            "55/55 [==============================] - 1s 23ms/step - loss: 2959.1145 - val_loss: 2938.0991 - lr: 0.0100\n",
            "Epoch 73/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 2917.4988 - val_loss: 2922.3860 - lr: 0.0100\n",
            "Epoch 74/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 2922.6621 - val_loss: 2948.7886 - lr: 0.0100\n",
            "Epoch 75/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 2919.1702 - val_loss: 2912.6355 - lr: 0.0100\n",
            "Epoch 76/120\n",
            "55/55 [==============================] - 2s 38ms/step - loss: 2916.3906 - val_loss: 2948.9512 - lr: 0.0100\n",
            "Epoch 77/120\n",
            "55/55 [==============================] - 2s 34ms/step - loss: 2910.8140 - val_loss: 3014.3025 - lr: 0.0100\n",
            "Epoch 78/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 2934.5835 - val_loss: 3156.9387 - lr: 0.0100\n",
            "Epoch 79/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 2949.3638 - val_loss: 2925.9255 - lr: 0.0100\n",
            "Epoch 80/120\n",
            "55/55 [==============================] - 1s 23ms/step - loss: 2920.7917 - val_loss: 2916.3342 - lr: 0.0100\n",
            "Epoch 81/120\n",
            "55/55 [==============================] - 1s 23ms/step - loss: 2925.9680 - val_loss: 3055.0288 - lr: 0.0100\n",
            "Epoch 82/120\n",
            "55/55 [==============================] - 1s 23ms/step - loss: 2918.4119 - val_loss: 2936.8237 - lr: 0.0100\n",
            "Epoch 83/120\n",
            "55/55 [==============================] - 1s 21ms/step - loss: 2929.1880 - val_loss: 2911.6057 - lr: 0.0100\n",
            "Epoch 84/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 2921.9985 - val_loss: 2968.0991 - lr: 0.0100\n",
            "Epoch 85/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 2925.4375 - val_loss: 2916.8569 - lr: 0.0100\n",
            "Epoch 86/120\n",
            "55/55 [==============================] - 2s 33ms/step - loss: 2903.6709 - val_loss: 2901.9255 - lr: 0.0095\n",
            "Epoch 87/120\n",
            "55/55 [==============================] - 2s 37ms/step - loss: 2903.4893 - val_loss: 2920.4214 - lr: 0.0090\n",
            "Epoch 88/120\n",
            "55/55 [==============================] - 1s 23ms/step - loss: 2892.7190 - val_loss: 2934.9900 - lr: 0.0086\n",
            "Epoch 89/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 2897.4890 - val_loss: 2935.6912 - lr: 0.0082\n",
            "Epoch 90/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 2894.4646 - val_loss: 2999.7517 - lr: 0.0078\n",
            "Epoch 91/120\n",
            "55/55 [==============================] - 1s 23ms/step - loss: 2892.4695 - val_loss: 3034.5371 - lr: 0.0074\n",
            "Epoch 92/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 2889.6365 - val_loss: 2914.3394 - lr: 0.0070\n",
            "Epoch 93/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 2872.8757 - val_loss: 2962.6555 - lr: 0.0067\n",
            "Epoch 94/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 2882.6882 - val_loss: 2909.3308 - lr: 0.0064\n",
            "Epoch 95/120\n",
            "55/55 [==============================] - 1s 23ms/step - loss: 2870.8213 - val_loss: 2868.6963 - lr: 0.0061\n",
            "Epoch 96/120\n",
            "55/55 [==============================] - 2s 33ms/step - loss: 2866.6865 - val_loss: 2991.9519 - lr: 0.0058\n",
            "Epoch 97/120\n",
            "55/55 [==============================] - 2s 38ms/step - loss: 2872.8210 - val_loss: 2884.8181 - lr: 0.0055\n",
            "Epoch 98/120\n",
            "55/55 [==============================] - 1s 24ms/step - loss: 2863.5425 - val_loss: 2881.5938 - lr: 0.0052\n",
            "Epoch 99/120\n",
            "55/55 [==============================] - 1s 23ms/step - loss: 2860.0215 - val_loss: 2866.7961 - lr: 0.0050\n",
            "Epoch 100/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 2858.3484 - val_loss: 2879.4419 - lr: 0.0047\n",
            "Epoch 101/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 2854.8594 - val_loss: 2870.6846 - lr: 0.0045\n",
            "Epoch 102/120\n",
            "55/55 [==============================] - 1s 23ms/step - loss: 2859.9497 - val_loss: 2864.5571 - lr: 0.0043\n",
            "Epoch 103/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 2854.7112 - val_loss: 2887.6846 - lr: 0.0041\n",
            "Epoch 104/120\n",
            "55/55 [==============================] - 1s 23ms/step - loss: 2869.1064 - val_loss: 2859.8657 - lr: 0.0039\n",
            "Epoch 105/120\n",
            "55/55 [==============================] - 1s 23ms/step - loss: 2848.3359 - val_loss: 2899.4187 - lr: 0.0037\n",
            "Epoch 106/120\n",
            "55/55 [==============================] - 2s 32ms/step - loss: 2851.1072 - val_loss: 2871.0073 - lr: 0.0035\n",
            "Epoch 107/120\n",
            "55/55 [==============================] - 2s 35ms/step - loss: 2849.2039 - val_loss: 2847.8191 - lr: 0.0033\n",
            "Epoch 108/120\n",
            "55/55 [==============================] - 2s 27ms/step - loss: 2840.7231 - val_loss: 2871.0247 - lr: 0.0032\n",
            "Epoch 109/120\n",
            "55/55 [==============================] - 1s 23ms/step - loss: 2843.0466 - val_loss: 2867.0334 - lr: 0.0030\n",
            "Epoch 110/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 2841.9604 - val_loss: 2850.0022 - lr: 0.0029\n",
            "Epoch 111/120\n",
            "55/55 [==============================] - 1s 23ms/step - loss: 2836.6348 - val_loss: 2841.5693 - lr: 0.0027\n",
            "Epoch 112/120\n",
            "55/55 [==============================] - 2s 31ms/step - loss: 2840.8428 - val_loss: 2847.5518 - lr: 0.0026\n",
            "Epoch 113/120\n",
            "55/55 [==============================] - 1s 23ms/step - loss: 2838.0737 - val_loss: 2874.3865 - lr: 0.0025\n",
            "Epoch 114/120\n",
            "55/55 [==============================] - 1s 23ms/step - loss: 2836.2498 - val_loss: 2840.6189 - lr: 0.0023\n",
            "Epoch 115/120\n",
            "55/55 [==============================] - 1s 26ms/step - loss: 2833.1655 - val_loss: 2851.4866 - lr: 0.0022\n",
            "Epoch 116/120\n",
            "55/55 [==============================] - 2s 37ms/step - loss: 2836.6082 - val_loss: 2847.8289 - lr: 0.0021\n",
            "Epoch 117/120\n",
            "55/55 [==============================] - 2s 35ms/step - loss: 2834.5251 - val_loss: 2844.4609 - lr: 0.0020\n",
            "Epoch 118/120\n",
            "55/55 [==============================] - 1s 23ms/step - loss: 2832.8127 - val_loss: 2833.2153 - lr: 0.0019\n",
            "Epoch 119/120\n",
            "55/55 [==============================] - 1s 23ms/step - loss: 2830.0608 - val_loss: 2842.4685 - lr: 0.0018\n",
            "Epoch 120/120\n",
            "55/55 [==============================] - 1s 22ms/step - loss: 2828.8918 - val_loss: 2834.0454 - lr: 0.0017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callback_upper = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "model_upper = keras.Sequential()\n",
        "\n",
        "model_upper.add(keras.Input(shape = (len(X_train[0]), ) ))\n",
        "model_upper.add(layers.Dense(64, activation=\"relu\"))\n",
        "model_upper.add(layers.Dense(32, activation=\"relu\"))\n",
        "model_upper.add(layers.Dense(16, activation=\"relu\"))\n",
        "model_upper.add(layers.Dense(8, activation=\"relu\"))\n",
        "model_upper.add(layers.Dense(8, activation=\"relu\"))\n",
        "model_upper.add(layers.Dense(8, activation=\"relu\"))\n",
        "model_upper.add(layers.Dense(1, activation=\"linear\"))\n",
        "\n",
        "\n",
        "opt_upper = keras.optimizers.Adam(learning_rate=0.01)\n",
        "model_upper.compile(loss=\"mse\", optimizer=opt_upper)\n",
        "history_upper = model_upper.fit(X_train_upper, y_train_upper, epochs=epochs, batch_size=batch_size,\n",
        "                    callbacks=[callback_upper], validation_data=(X_test_upper, y_test_upper))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpux5U9tP5K2",
        "outputId": "39fc88d8-65ee-414f-be8a-fe4beb84a626"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/120\n",
            "16/16 [==============================] - 2s 35ms/step - loss: 53168228.0000 - val_loss: 51434212.0000 - lr: 0.0100\n",
            "Epoch 2/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 47183676.0000 - val_loss: 46165344.0000 - lr: 0.0100\n",
            "Epoch 3/120\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 44550100.0000 - val_loss: 43531888.0000 - lr: 0.0100\n",
            "Epoch 4/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 41983552.0000 - val_loss: 40251920.0000 - lr: 0.0100\n",
            "Epoch 5/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 38722472.0000 - val_loss: 36828780.0000 - lr: 0.0100\n",
            "Epoch 6/120\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 35365708.0000 - val_loss: 34064112.0000 - lr: 0.0100\n",
            "Epoch 7/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 32442562.0000 - val_loss: 31064264.0000 - lr: 0.0100\n",
            "Epoch 8/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 32041938.0000 - val_loss: 31660056.0000 - lr: 0.0100\n",
            "Epoch 9/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 31160420.0000 - val_loss: 30843762.0000 - lr: 0.0100\n",
            "Epoch 10/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 30579774.0000 - val_loss: 30413354.0000 - lr: 0.0100\n",
            "Epoch 11/120\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 30296982.0000 - val_loss: 30382690.0000 - lr: 0.0100\n",
            "Epoch 12/120\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 30320790.0000 - val_loss: 30308514.0000 - lr: 0.0100\n",
            "Epoch 13/120\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 30273230.0000 - val_loss: 30297976.0000 - lr: 0.0100\n",
            "Epoch 14/120\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 30300698.0000 - val_loss: 30430876.0000 - lr: 0.0100\n",
            "Epoch 15/120\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 30132454.0000 - val_loss: 30242008.0000 - lr: 0.0100\n",
            "Epoch 16/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 29988112.0000 - val_loss: 30117506.0000 - lr: 0.0100\n",
            "Epoch 17/120\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 29866734.0000 - val_loss: 30140824.0000 - lr: 0.0100\n",
            "Epoch 18/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 29927762.0000 - val_loss: 30055296.0000 - lr: 0.0100\n",
            "Epoch 19/120\n",
            "16/16 [==============================] - 1s 35ms/step - loss: 29804494.0000 - val_loss: 29958820.0000 - lr: 0.0100\n",
            "Epoch 20/120\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 29935048.0000 - val_loss: 30115962.0000 - lr: 0.0100\n",
            "Epoch 21/120\n",
            "16/16 [==============================] - 1s 40ms/step - loss: 29839002.0000 - val_loss: 29892142.0000 - lr: 0.0100\n",
            "Epoch 22/120\n",
            "16/16 [==============================] - 1s 40ms/step - loss: 29686074.0000 - val_loss: 29880196.0000 - lr: 0.0100\n",
            "Epoch 23/120\n",
            "16/16 [==============================] - 1s 38ms/step - loss: 29642916.0000 - val_loss: 30055342.0000 - lr: 0.0100\n",
            "Epoch 24/120\n",
            "16/16 [==============================] - 1s 39ms/step - loss: 29534720.0000 - val_loss: 29733300.0000 - lr: 0.0100\n",
            "Epoch 25/120\n",
            "16/16 [==============================] - 1s 31ms/step - loss: 30035558.0000 - val_loss: 30669000.0000 - lr: 0.0100\n",
            "Epoch 26/120\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 30322928.0000 - val_loss: 29816076.0000 - lr: 0.0100\n",
            "Epoch 27/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 29129346.0000 - val_loss: 28762694.0000 - lr: 0.0100\n",
            "Epoch 28/120\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 28170206.0000 - val_loss: 28652426.0000 - lr: 0.0100\n",
            "Epoch 29/120\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 27837778.0000 - val_loss: 28223420.0000 - lr: 0.0100\n",
            "Epoch 30/120\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 27803358.0000 - val_loss: 28631830.0000 - lr: 0.0100\n",
            "Epoch 31/120\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 27935602.0000 - val_loss: 27820702.0000 - lr: 0.0100\n",
            "Epoch 32/120\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 27386882.0000 - val_loss: 27559952.0000 - lr: 0.0100\n",
            "Epoch 33/120\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 27074096.0000 - val_loss: 27408020.0000 - lr: 0.0100\n",
            "Epoch 34/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 26899648.0000 - val_loss: 27406818.0000 - lr: 0.0100\n",
            "Epoch 35/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 26883942.0000 - val_loss: 27303510.0000 - lr: 0.0100\n",
            "Epoch 36/120\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 26772450.0000 - val_loss: 27226860.0000 - lr: 0.0100\n",
            "Epoch 37/120\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 26657262.0000 - val_loss: 27105350.0000 - lr: 0.0100\n",
            "Epoch 38/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 26519810.0000 - val_loss: 27103386.0000 - lr: 0.0100\n",
            "Epoch 39/120\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 26514840.0000 - val_loss: 26971436.0000 - lr: 0.0100\n",
            "Epoch 40/120\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 26388592.0000 - val_loss: 27048180.0000 - lr: 0.0100\n",
            "Epoch 41/120\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 26400352.0000 - val_loss: 26842014.0000 - lr: 0.0100\n",
            "Epoch 42/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 26318360.0000 - val_loss: 26964698.0000 - lr: 0.0100\n",
            "Epoch 43/120\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 26181394.0000 - val_loss: 26807524.0000 - lr: 0.0100\n",
            "Epoch 44/120\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 26193734.0000 - val_loss: 26683494.0000 - lr: 0.0100\n",
            "Epoch 45/120\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 25998024.0000 - val_loss: 26801908.0000 - lr: 0.0100\n",
            "Epoch 46/120\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 26006368.0000 - val_loss: 26643910.0000 - lr: 0.0100\n",
            "Epoch 47/120\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 25863066.0000 - val_loss: 26728398.0000 - lr: 0.0100\n",
            "Epoch 48/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 25798168.0000 - val_loss: 26942398.0000 - lr: 0.0100\n",
            "Epoch 49/120\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 25759560.0000 - val_loss: 26740276.0000 - lr: 0.0100\n",
            "Epoch 50/120\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 25667316.0000 - val_loss: 26593650.0000 - lr: 0.0100\n",
            "Epoch 51/120\n",
            "16/16 [==============================] - 1s 38ms/step - loss: 25624748.0000 - val_loss: 26689816.0000 - lr: 0.0100\n",
            "Epoch 52/120\n",
            "16/16 [==============================] - 1s 38ms/step - loss: 25570794.0000 - val_loss: 26634046.0000 - lr: 0.0100\n",
            "Epoch 53/120\n",
            "16/16 [==============================] - 1s 40ms/step - loss: 25500638.0000 - val_loss: 26502258.0000 - lr: 0.0100\n",
            "Epoch 54/120\n",
            "16/16 [==============================] - 1s 40ms/step - loss: 25564990.0000 - val_loss: 26371410.0000 - lr: 0.0100\n",
            "Epoch 55/120\n",
            "16/16 [==============================] - 1s 38ms/step - loss: 25365176.0000 - val_loss: 26564224.0000 - lr: 0.0100\n",
            "Epoch 56/120\n",
            "16/16 [==============================] - 1s 39ms/step - loss: 25417592.0000 - val_loss: 26535586.0000 - lr: 0.0100\n",
            "Epoch 57/120\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 25494916.0000 - val_loss: 26460564.0000 - lr: 0.0100\n",
            "Epoch 58/120\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 25693064.0000 - val_loss: 26492414.0000 - lr: 0.0100\n",
            "Epoch 59/120\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 25445476.0000 - val_loss: 26494602.0000 - lr: 0.0100\n",
            "Epoch 60/120\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 25341780.0000 - val_loss: 26370682.0000 - lr: 0.0100\n",
            "Epoch 61/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 25403374.0000 - val_loss: 26685444.0000 - lr: 0.0100\n",
            "Epoch 62/120\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 25372344.0000 - val_loss: 26542026.0000 - lr: 0.0100\n",
            "Epoch 63/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 25502272.0000 - val_loss: 26572592.0000 - lr: 0.0100\n",
            "Epoch 64/120\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 25061150.0000 - val_loss: 26223378.0000 - lr: 0.0100\n",
            "Epoch 65/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 25116938.0000 - val_loss: 26752588.0000 - lr: 0.0100\n",
            "Epoch 66/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 25158006.0000 - val_loss: 26007570.0000 - lr: 0.0100\n",
            "Epoch 67/120\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 25085452.0000 - val_loss: 26017374.0000 - lr: 0.0100\n",
            "Epoch 68/120\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 24922394.0000 - val_loss: 26215502.0000 - lr: 0.0100\n",
            "Epoch 69/120\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 25025874.0000 - val_loss: 26344798.0000 - lr: 0.0100\n",
            "Epoch 70/120\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 24840046.0000 - val_loss: 25948304.0000 - lr: 0.0100\n",
            "Epoch 71/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 25097172.0000 - val_loss: 27420382.0000 - lr: 0.0100\n",
            "Epoch 72/120\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 25210878.0000 - val_loss: 26538742.0000 - lr: 0.0100\n",
            "Epoch 73/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 24895420.0000 - val_loss: 26197220.0000 - lr: 0.0100\n",
            "Epoch 74/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 24802312.0000 - val_loss: 26371220.0000 - lr: 0.0100\n",
            "Epoch 75/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 24864322.0000 - val_loss: 26207552.0000 - lr: 0.0100\n",
            "Epoch 76/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 24892532.0000 - val_loss: 26585518.0000 - lr: 0.0100\n",
            "Epoch 77/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 24846816.0000 - val_loss: 26288586.0000 - lr: 0.0100\n",
            "Epoch 78/120\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 24667342.0000 - val_loss: 26145450.0000 - lr: 0.0100\n",
            "Epoch 79/120\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 24730736.0000 - val_loss: 26209936.0000 - lr: 0.0100\n",
            "Epoch 80/120\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 24843334.0000 - val_loss: 26400246.0000 - lr: 0.0100\n",
            "Epoch 81/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 24668962.0000 - val_loss: 26903410.0000 - lr: 0.0100\n",
            "Epoch 82/120\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 24870484.0000 - val_loss: 26166742.0000 - lr: 0.0100\n",
            "Epoch 83/120\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 24843072.0000 - val_loss: 26373786.0000 - lr: 0.0100\n",
            "Epoch 84/120\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 24836260.0000 - val_loss: 26536498.0000 - lr: 0.0100\n",
            "Epoch 85/120\n",
            "16/16 [==============================] - 1s 38ms/step - loss: 24591668.0000 - val_loss: 26371440.0000 - lr: 0.0100\n",
            "Epoch 86/120\n",
            "16/16 [==============================] - 1s 40ms/step - loss: 24486900.0000 - val_loss: 26283056.0000 - lr: 0.0095\n",
            "Epoch 87/120\n",
            "16/16 [==============================] - 1s 41ms/step - loss: 24566056.0000 - val_loss: 26161470.0000 - lr: 0.0090\n",
            "Epoch 88/120\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 24497234.0000 - val_loss: 26221332.0000 - lr: 0.0086\n",
            "Epoch 89/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 24624686.0000 - val_loss: 26357768.0000 - lr: 0.0082\n",
            "Epoch 90/120\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 24462956.0000 - val_loss: 26116412.0000 - lr: 0.0078\n",
            "Epoch 91/120\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 24423996.0000 - val_loss: 25998844.0000 - lr: 0.0074\n",
            "Epoch 92/120\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 24282596.0000 - val_loss: 26302236.0000 - lr: 0.0070\n",
            "Epoch 93/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 24320380.0000 - val_loss: 25996100.0000 - lr: 0.0067\n",
            "Epoch 94/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 24288052.0000 - val_loss: 26067258.0000 - lr: 0.0064\n",
            "Epoch 95/120\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 24190756.0000 - val_loss: 26156136.0000 - lr: 0.0061\n",
            "Epoch 96/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 24163490.0000 - val_loss: 26348562.0000 - lr: 0.0058\n",
            "Epoch 97/120\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 24135910.0000 - val_loss: 26197934.0000 - lr: 0.0055\n",
            "Epoch 98/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 24027252.0000 - val_loss: 26232382.0000 - lr: 0.0052\n",
            "Epoch 99/120\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 24058222.0000 - val_loss: 26001852.0000 - lr: 0.0050\n",
            "Epoch 100/120\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 24003816.0000 - val_loss: 26152578.0000 - lr: 0.0047\n",
            "Epoch 101/120\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 23977234.0000 - val_loss: 26267336.0000 - lr: 0.0045\n",
            "Epoch 102/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 23974154.0000 - val_loss: 26183968.0000 - lr: 0.0043\n",
            "Epoch 103/120\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 24019548.0000 - val_loss: 26127300.0000 - lr: 0.0041\n",
            "Epoch 104/120\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 23972780.0000 - val_loss: 26116708.0000 - lr: 0.0039\n",
            "Epoch 105/120\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 23899332.0000 - val_loss: 26134544.0000 - lr: 0.0037\n",
            "Epoch 106/120\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 23913562.0000 - val_loss: 26085918.0000 - lr: 0.0035\n",
            "Epoch 107/120\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 23868568.0000 - val_loss: 26248852.0000 - lr: 0.0033\n",
            "Epoch 108/120\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 23862142.0000 - val_loss: 26200482.0000 - lr: 0.0032\n",
            "Epoch 109/120\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 23910560.0000 - val_loss: 26183364.0000 - lr: 0.0030\n",
            "Epoch 110/120\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 23870710.0000 - val_loss: 26098114.0000 - lr: 0.0029\n",
            "Epoch 111/120\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 23809674.0000 - val_loss: 26232550.0000 - lr: 0.0027\n",
            "Epoch 112/120\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 23836926.0000 - val_loss: 26230314.0000 - lr: 0.0026\n",
            "Epoch 113/120\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 23829312.0000 - val_loss: 26073892.0000 - lr: 0.0025\n",
            "Epoch 114/120\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 23787930.0000 - val_loss: 26150318.0000 - lr: 0.0023\n",
            "Epoch 115/120\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 23769524.0000 - val_loss: 26223786.0000 - lr: 0.0022\n",
            "Epoch 116/120\n",
            "16/16 [==============================] - 1s 38ms/step - loss: 23775212.0000 - val_loss: 26157424.0000 - lr: 0.0021\n",
            "Epoch 117/120\n",
            "16/16 [==============================] - 1s 39ms/step - loss: 23796262.0000 - val_loss: 26235720.0000 - lr: 0.0020\n",
            "Epoch 118/120\n",
            "16/16 [==============================] - 1s 41ms/step - loss: 23722392.0000 - val_loss: 26231224.0000 - lr: 0.0019\n",
            "Epoch 119/120\n",
            "16/16 [==============================] - 1s 40ms/step - loss: 23708420.0000 - val_loss: 26134838.0000 - lr: 0.0018\n",
            "Epoch 120/120\n",
            "16/16 [==============================] - 1s 35ms/step - loss: 23694898.0000 - val_loss: 26141128.0000 - lr: 0.0017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_lower = model_lower.predict(X_test_lower)\n",
        "y_pred_upper = model_upper.predict(X_test_upper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8sMYu-8P-P0",
        "outputId": "b140500c-c58b-4ead-8cf5-c9f9fcca07be"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9226/9226 [==============================] - 14s 2ms/step\n",
            "2728/2728 [==============================] - 4s 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r2_lower = r2_score(y_test_lower, y_pred_lower)\n",
        "mae_lower = mean_absolute_error(y_test_lower, y_pred_lower)\n",
        "mse_lower = mean_squared_error(y_test_lower, y_pred_lower)\n",
        "rmse_lower = mean_squared_error(y_test_lower, y_pred_lower, squared=False)\n",
        "\n",
        "print('lower r2 = %.3f' % r2_lower)\n",
        "print('lower mae = %.3f' % mae_lower)\n",
        "print('lower mse = %.3f' % mse_lower)\n",
        "print('lower rmse = %.3f' % rmse_lower)\n",
        "\n",
        "\n",
        "print(y_test_upper.shape)\n",
        "print(y_pred_upper.shape)\n",
        "\n",
        "r2_upper = r2_score(y_test_upper, y_pred_upper)\n",
        "mae_upper = mean_absolute_error(y_test_upper, y_pred_upper)\n",
        "mse_upper = mean_squared_error(y_test_upper, y_pred_upper)\n",
        "rmse_upper = mean_squared_error(y_test_upper, y_pred_upper, squared=False)\n",
        "\n",
        "print('upper r2 = %.3f' % r2_upper)\n",
        "print('upper mae = %.3f' % mae_upper)\n",
        "print('upper mse = %.3f' % mse_upper)\n",
        "print('upper rmse = %.3f' % rmse_upper)"
      ],
      "metadata": {
        "id": "fFzv5_-rNz6t",
        "outputId": "9e00857f-f542-48c3-9091-14b6b700cfa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lower r2 = 0.696\n",
            "lower mae = 29.770\n",
            "lower mse = 2834.046\n",
            "lower rmse = 53.236\n",
            "(87277,)\n",
            "(87277, 1)\n",
            "upper r2 = 0.452\n",
            "upper mae = 1539.097\n",
            "upper mse = 26141128.991\n",
            "upper rmse = 5112.840\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now combine the breakpoint calculator with the two models to make predictions for all data\n",
        "\n",
        "def classify_and_predict(X):\n",
        "    # threshhold of\n",
        "    predicted_upper_indices = (model_breakpoint.predict(X) > 0.5)\n",
        "    predicted_lower_indices = np.invert(predicted_upper_indices)\n",
        "\n",
        "    predicted_upper = X[predicted_upper_indices[:, 0]]\n",
        "    predicted_lower = X[predicted_lower_indices[:, 0]]\n",
        "\n",
        "    upper_results = model_upper.predict(predicted_upper)\n",
        "    lower_results = model_lower.predict(predicted_lower)\n",
        "\n",
        "    results = np.empty(shape=(X.shape[0], 1))\n",
        "\n",
        "    results[predicted_upper_indices[:, 0]] = upper_results\n",
        "    results[predicted_lower_indices[:, 0]] = lower_results\n",
        "\n",
        "    return results\n",
        "\n",
        "X_test_concat = np.vstack((X_test_lower, X_test_upper))\n",
        "y_test_concat = np.hstack((y_test_lower, y_test_upper))\n",
        "\n",
        "y_pred_concat = classify_and_predict(X_test_concat)"
      ],
      "metadata": {
        "id": "GR4BQPKkQcMF",
        "outputId": "3fca8e7c-ff72-4727-dc12-998314c8f682",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11954/11954 [==============================] - 19s 2ms/step\n",
            "2228/2228 [==============================] - 3s 1ms/step\n",
            "9726/9726 [==============================] - 15s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r2 = r2_score(y_test_concat, y_pred_concat)\n",
        "mae = mean_absolute_error(y_test_concat, y_pred_concat)\n",
        "mse = mean_squared_error(y_test_concat, y_pred_concat)\n",
        "rmse = mean_squared_error(y_test_concat, y_pred_concat, squared=False)\n",
        "\n",
        "print('r2 = %.3f' % r2)\n",
        "print('mae = %.3f' % mae)\n",
        "print('mse = %.3f' % mse)\n",
        "print('rmse = %.3f' % rmse)"
      ],
      "metadata": {
        "id": "4HjrIKx8XL-5",
        "outputId": "b1e99d60-8938-43f1-e4cf-7d68ec1ca61e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "r2 = 0.493\n",
            "mae = 395.742\n",
            "mse = 6041936.980\n",
            "rmse = 2458.035\n"
          ]
        }
      ]
    }
  ]
}